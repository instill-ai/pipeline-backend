{
  "$defs": {
    "chat-message": {
      "properties": {
        "content": {
          "$ref": "https://raw.githubusercontent.com/instill-ai/component/467caa4c05cf75d88e2036555529ecf6aa163b5c/resources/schemas/schema.json#/$defs/instill-types/multi-modal-content",
          "description": "The message content",
          "instillUIOrder": 1,
          "title": "Content"
        },
        "role": {
          "description": "The message role, i.e. 'system', 'user' or 'assistant'",
          "instillFormat": "string",
          "instillUIOrder": 0,
          "title": "Role",
          "type": "string"
        }
      },
      "required": [
        "role",
        "content"
      ],
      "title": "Chat Message",
      "type": "object"
    }
  },
  "TASK_SPEECH_RECOGNITION": {
    "instillShortDescription": "Turn audio into text.",
    "input": {
      "instillUIOrder": 0,
      "properties": {
        "audio": {
          "description": "The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n",
          "format": "binary",
          "type": "string",
          "instillAcceptFormats": [
            "audio/*"
          ],
          "instillUIOrder": 1,
          "instillUpstreamTypes": [
            "reference"
          ],
          "title": "Audio"
        },
        "language": {
          "description": "The language of the input audio. Supplying the input language in <a href=\"https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\">ISO-639-1</a> format will improve accuracy and latency.\n",
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "The language of the input audio.",
          "instillUIOrder": 3,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Language"
        },
        "model": {
          "description": "ID of the model to use. Only `whisper-1` is currently available.\n",
          "enum": [
            "whisper-1"
          ],
          "example": "whisper-1",
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "ID of the model to use",
          "instillUIOrder": 0,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "instillCredentialMap": {
            "values": [
              "whisper-1"
            ],
            "targets": [
              "setup.api-key"
            ]
          },
          "title": "Model"
        },
        "prompt": {
          "description": "An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.\n",
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "An optional text to guide the model's style or continue a previous audio segment.",
          "instillUIMultiline": true,
          "instillUIOrder": 2,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Prompt"
        },
        "temperature": {
          "default": 0,
          "description": "The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use <a href=\"https://en.wikipedia.org/wiki/Log_probability\">log probability</a> to automatically increase the temperature until certain thresholds are hit.\n",
          "type": "number",
          "instillAcceptFormats": [
            "number",
            "integer"
          ],
          "instillShortDescription": "The sampling temperature, between 0 and 1.",
          "instillUIOrder": 4,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "Temperature"
        }
      },
      "required": [
        "audio",
        "model"
      ],
      "title": "Input",
      "type": "object"
    },
    "output": {
      "instillUIOrder": 0,
      "properties": {
        "text": {
          "type": "string",
          "description": "Generated text",
          "instillFormat": "string",
          "instillUIOrder": 0,
          "title": "Text"
        }
      },
      "required": [
        "text"
      ],
      "title": "Output",
      "type": "object"
    }
  },
  "TASK_TEXT_EMBEDDINGS": {
    "instillShortDescription": "Turn text into numbers, unlocking use cases like search.",
    "input": {
      "instillUIOrder": 0,
      "properties": {
        "model": {
          "description": "ID of the model to use.",
          "enum": [
            "text-embedding-ada-002",
            "text-embedding-3-small",
            "text-embedding-3-large"
          ],
          "example": "text-embedding-3-small",
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "ID of the model to use",
          "instillUIOrder": 0,
          "instillCredentialMap": {
            "values": [
              "text-embedding-3-small",
              "text-embedding-3-large"
            ],
            "targets": [
              "setup.api-key"
            ]
          },
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Model"
        },
        "text": {
          "description": "The text",
          "instillAcceptFormats": [
            "string"
          ],
          "instillUIMultiline": true,
          "instillUIOrder": 1,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Text",
          "type": "string"
        },
        "dimensions": {
          "description": "The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models.",
          "instillAcceptFormats": [
            "integer"
          ],
          "instillUIOrder": 2,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "Dimensions",
          "type": "integer"
        }
      },
      "required": [
        "text",
        "model"
      ],
      "title": "Input",
      "type": "object"
    },
    "output": {
      "instillUIOrder": 0,
      "properties": {
        "embedding": {
          "$ref": "https://raw.githubusercontent.com/instill-ai/component/467caa4c05cf75d88e2036555529ecf6aa163b5c/resources/schemas/schema.json#/$defs/instill-types/embedding",
          "description": "Embedding of the input text",
          "instillUIOrder": 0,
          "title": "Embedding"
        }
      },
      "required": [
        "embedding"
      ],
      "title": "Output",
      "type": "object"
    }
  },
  "TASK_TEXT_GENERATION": {
    "instillShortDescription": "Provide text outputs in response to their inputs.",
    "description": "OpenAI's text generation models (often called generative pre-trained transformers or large language models) have been trained to understand natural language, code, and images. The models provide text outputs in response to their inputs. The inputs to these models are also referred to as \"prompts\". Designing a prompt is essentially how you \u201cprogram\u201d a large language model model, usually by providing instructions or some examples of how to successfully complete a task.",
    "input": {
      "instillUIOrder": 0,
      "properties": {
        "chat-history": {
          "description": "Incorporate external chat history, specifically previous messages within the conversation. Please note that System Message will be ignored and will not have any effect when this field is populated. Each message should adhere to the format {\"role\": \"The message role, i.e. 'system', 'user' or 'assistant'\", \"content\": \"message content\"}.",
          "instillAcceptFormats": [
            "structured/chat-messages"
          ],
          "instillShortDescription": "Incorporate external chat history, specifically previous messages within the conversation. Please note that System Message will be ignored and will not have any effect when this field is populated. Each message should be an ojbect adhere to the format: {\"role\": \"The message role, i.e. 'system', 'user' or 'assistant'\", \"content\": \"message content\"}.",
          "instillUIOrder": 4,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "items": {
            "$ref": "#/$defs/chat-message"
          },
          "title": "Chat history",
          "type": "array"
        },
        "frequency-penalty": {
          "default": 0,
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
          "maximum": 2,
          "minimum": -2,
          "nullable": true,
          "type": "number",
          "instillAcceptFormats": [
            "number",
            "integer"
          ],
          "instillShortDescription": "Number between -2.0 and 2.0",
          "instillUIOrder": 11,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Frequency Penalty"
        },
        "images": {
          "description": "The images",
          "instillAcceptFormats": [
            "array:image/*"
          ],
          "instillUIOrder": 3,
          "instillUpstreamTypes": [
            "reference"
          ],
          "items": {
            "type": "string"
          },
          "title": "Image",
          "type": "array"
        },
        "max-tokens": {
          "description": "The maximum number of tokens that can be generated in the chat completion.\n\nThe total length of input tokens and generated tokens is limited by the model's context length.",
          "nullable": true,
          "type": "integer",
          "instillAcceptFormats": [
            "integer"
          ],
          "instillShortDescription": "The maximum number of tokens to generate in the chat completion.",
          "instillUIOrder": 7,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "Max Tokens"
        },
        "model": {
          "description": "ID of the model to use.",
          "enum": [
            "o1-preview",
            "o1-mini",
            "gpt-4o-mini",
            "gpt-4o",
            "gpt-4o-2024-05-13",
            "gpt-4o-2024-08-06",
            "gpt-4-turbo",
            "gpt-4-turbo-2024-04-09",
            "gpt-4-0125-preview",
            "gpt-4-turbo-preview",
            "gpt-4-1106-preview",
            "gpt-4-vision-preview",
            "gpt-4",
            "gpt-4-0314",
            "gpt-4-0613",
            "gpt-4-32k",
            "gpt-4-32k-0314",
            "gpt-4-32k-0613",
            "gpt-3.5-turbo",
            "gpt-3.5-turbo-16k",
            "gpt-3.5-turbo-0301",
            "gpt-3.5-turbo-0613",
            "gpt-3.5-turbo-1106",
            "gpt-3.5-turbo-0125",
            "gpt-3.5-turbo-16k-0613"
          ],
          "example": "gpt-4o",
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "ID of the model to use",
          "instillUIOrder": 0,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "instillCredentialMap": {
            "values": [
              "o1-preview",
              "o1-mini",
              "gpt-4o",
              "gpt-4o-2024-08-06",
              "gpt-4-turbo",
              "gpt-4-vision-preview",
              "gpt-4",
              "gpt-4-32k",
              "gpt-3.5-turbo",
              "gpt-4o-mini"
            ],
            "targets": [
              "setup.api-key"
            ]
          },
          "title": "Model"
        },
        "n": {
          "default": 1,
          "description": "How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.",
          "example": 1,
          "maximum": 128,
          "minimum": 1,
          "nullable": true,
          "type": "integer",
          "instillAcceptFormats": [
            "integer"
          ],
          "instillUIOrder": 6,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "N"
        },
        "presence-penalty": {
          "default": 0,
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
          "maximum": 2,
          "minimum": -2,
          "nullable": true,
          "type": "number",
          "instillAcceptFormats": [
            "number",
            "integer"
          ],
          "instillShortDescription": "Number between -2.0 and 2.0",
          "instillUIOrder": 10,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Presence Penalty"
        },
        "prompt": {
          "description": "The prompt text",
          "instillAcceptFormats": [
            "string"
          ],
          "instillUIMultiline": true,
          "instillUIOrder": 1,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Prompt",
          "type": "string"
        },
        "response-format": {
          "description": "Response format.",
          "instillUIOrder": 8,
          "additionalProperties": true,
          "type": "object",
          "required": [
            "type"
          ],
          "oneOf": [
            {
              "properties": {
                "type": {
                  "type": "string",
                  "const": "text",
                  "title": "Type",
                  "description": "Text",
                  "instillUIOrder": 0
                }
              },
              "required": [
                "type"
              ],
              "title": "Text",
              "type": "object"
            },
            {
              "properties": {
                "type": {
                  "type": "string",
                  "const": "json_object",
                  "title": "Type",
                  "description": "JSON Object",
                  "instillUIOrder": 0
                }
              },
              "required": [
                "type"
              ],
              "title": "JSON Object",
              "type": "object"
            },
            {
              "properties": {
                "type": {
                  "type": "string",
                  "const": "json_schema",
                  "title": "Type",
                  "description": "JSON Schema",
                  "instillUIOrder": 0
                },
                "json-schema": {
                  "description": "Set up the schema of the structured output.",
                  "type": "string",
                  "instillAcceptFormats": [
                    "string"
                  ],
                  "title": "JSON Schema",
                  "instillShortDescription": "Specify the schema of the structured output.",
                  "instillUIOrder": 1,
                  "instillUIMultiline": true,
                  "instillUpstreamTypes": [
                    "value",
                    "reference"
                  ]
                }
              },
              "required": [
                "type",
                "json-schema"
              ],
              "title": "JSON Schema",
              "type": "object"
            }
          ],
          "title": "Response Format"
        },
        "system-message": {
          "default": "You are a helpful assistant.",
          "description": "The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. By default, the model\u2019s behavior is using a generic message as \"You are a helpful assistant.\"",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "The system message helps set the behavior of the assistant",
          "instillUIMultiline": true,
          "instillUIOrder": 2,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "System Message",
          "type": "string"
        },
        "temperature": {
          "default": 1,
          "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top-p` but not both.\n",
          "example": 1,
          "maximum": 2,
          "minimum": 0,
          "nullable": true,
          "type": "number",
          "instillAcceptFormats": [
            "number",
            "integer"
          ],
          "instillShortDescription": "What sampling temperature to use, between 0 and 2.",
          "instillUIOrder": 5,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "Temperature"
        },
        "top-p": {
          "default": 1,
          "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n",
          "example": 1,
          "maximum": 1,
          "minimum": 0,
          "nullable": true,
          "type": "number",
          "instillAcceptFormats": [
            "number",
            "integer"
          ],
          "instillShortDescription": "An alternative to sampling with temperature, called nucleus sampling",
          "instillUIOrder": 9,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "Top P"
        }
      },
      "instillEditOnNodeFields": [
        "model",
        "prompt",
        "response-format"
      ],
      "required": [
        "model",
        "prompt"
      ],
      "title": "Input",
      "type": "object"
    },
    "output": {
      "instillUIOrder": 0,
      "properties": {
        "texts": {
          "instillUIOrder": 0,
          "instillFormat": "array:string",
          "items": {
            "instillFormat": "string",
            "instillUIMultiline": true,
            "title": "Text",
            "type": "string"
          },
          "description": "Texts",
          "title": "Texts",
          "type": "array"
        },
        "usage": {
          "description": "Usage statistics related to the query",
          "instillUIOrder": 1,
          "properties": {
            "total-tokens": {
              "title": "Total tokens",
              "description": "Total number of tokens used (prompt + completion)",
              "instillFormat": "integer",
              "instillUIOrder": 0,
              "type": "integer"
            },
            "completion-tokens": {
              "title": "Completion tokens",
              "description": "Total number of tokens used (completion)",
              "instillFormat": "integer",
              "instillUIOrder": 1,
              "type": "integer"
            },
            "prompt-tokens": {
              "title": "Prompt tokens",
              "description": "Total number of tokens used (prompt)",
              "instillFormat": "integer",
              "instillUIOrder": 2,
              "type": "integer"
            }
          },
          "required": [
            "total-tokens"
          ],
          "title": "Usage",
          "type": "object"
        }
      },
      "required": [
        "texts"
      ],
      "title": "Output",
      "type": "object"
    }
  },
  "TASK_TEXT_TO_IMAGE": {
    "instillShortDescription": "Generate or manipulate images with DALL\u00b7E.",
    "input": {
      "instillUIOrder": 0,
      "properties": {
        "model": {
          "default": "dall-e-2",
          "description": "The model to use for image generation.",
          "enum": [
            "dall-e-2",
            "dall-e-3"
          ],
          "example": "dall-e-3",
          "nullable": true,
          "type": "string",
          "instillCredentialMap": {
            "values": [
              "dall-e-3"
            ],
            "targets": [
              "setup.api-key"
            ]
          },
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "ID of the model to use",
          "instillUIOrder": 0,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Model"
        },
        "n": {
          "default": 1,
          "description": "The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only `n=1` is supported.",
          "example": 1,
          "maximum": 10,
          "minimum": 1,
          "nullable": true,
          "type": "integer",
          "instillAcceptFormats": [
            "integer"
          ],
          "instillUIOrder": 2,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "N"
        },
        "prompt": {
          "description": "A text description of the desired image(s). The maximum length is 1000 characters for `dall-e-2` and 4000 characters for `dall-e-3`.",
          "example": "A cute baby sea otter",
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "A text description of the desired image(s).",
          "instillUIMultiline": true,
          "instillUIOrder": 1,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Prompt"
        },
        "quality": {
          "default": "standard",
          "description": "The quality of the image that will be generated. `hd` creates images with finer details and greater consistency across the image. This param is only supported for `dall-e-3`.",
          "enum": [
            "standard",
            "hd"
          ],
          "example": "standard",
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "The quality of the image that will be generated.",
          "instillUIOrder": 3,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "Quality"
        },
        "size": {
          "default": "1024x1024",
          "description": "The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3` models.",
          "enum": [
            "256x256",
            "512x512",
            "1024x1024",
            "1792x1024",
            "1024x1792"
          ],
          "example": "1024x1024",
          "nullable": true,
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "The size of the generated images.",
          "instillUIOrder": 4,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "Size"
        },
        "style": {
          "default": "vivid",
          "description": "The style of the generated images. Must be one of `vivid` or `natural`. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for `dall-e-3`.",
          "enum": [
            "vivid",
            "natural"
          ],
          "example": "vivid",
          "nullable": true,
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "The style of the generated images.",
          "instillUIOrder": 5,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "N"
        }
      },
      "required": [
        "prompt",
        "model"
      ],
      "title": "Input",
      "type": "object"
    },
    "output": {
      "instillUIOrder": 0,
      "properties": {
        "results": {
          "description": "Generated results",
          "instillUIOrder": 0,
          "items": {
            "description": "Generated result",
            "properties": {
              "image": {
                "title": "Generated Image",
                "description": "Generated image",
                "instillFormat": "image/webp",
                "instillUIOrder": 0,
                "type": "string"
              },
              "revised-prompt": {
                "title": "Revised Prompt",
                "description": "Revised prompt",
                "instillFormat": "string",
                "instillUIMultiline": true,
                "instillUIOrder": 1,
                "type": "string"
              }
            },
            "required": [
              "image",
              "revised-prompt"
            ],
            "title": "Image",
            "type": "object"
          },
          "title": "Images",
          "type": "array"
        }
      },
      "required": [
        "results"
      ],
      "title": "Output",
      "type": "object"
    }
  },
  "TASK_TEXT_TO_SPEECH": {
    "instillShortDescription": "Turn text into lifelike spoken audio",
    "input": {
      "instillUIOrder": 0,
      "properties": {
        "model": {
          "description": "One of the available TTS models: `tts-1` or `tts-1-hd`\n",
          "default": "tts-1",
          "enum": [
            "tts-1",
            "tts-1-hd"
          ],
          "instillAcceptFormats": [
            "string"
          ],
          "instillCredentialMap": {
            "values": [
              "tts-1",
              "tts-1-hd"
            ],
            "targets": [
              "setup.api-key"
            ]
          },
          "instillShortDescription": "ID of the model to use",
          "instillUIOrder": 0,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Model",
          "type": "string"
        },
        "response-format": {
          "default": "mp3",
          "description": "The format to audio in. Supported formats are `mp3`, `opus`, `aac`, and `flac`.",
          "enum": [
            "mp3",
            "opus",
            "aac",
            "flac"
          ],
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "The format to audio",
          "instillUIOrder": 3,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "Response Format"
        },
        "speed": {
          "default": 1,
          "description": "The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is the default.",
          "maximum": 4,
          "minimum": 0.25,
          "type": "number",
          "instillAcceptFormats": [
            "number"
          ],
          "instillShortDescription": "The speed of the generated audio",
          "instillUIOrder": 4,
          "instillUpstreamTypes": [
            "value",
            "reference"
          ],
          "title": "Speed"
        },
        "text": {
          "description": "The text to generate audio for. The maximum length is 4096 characters.",
          "maxLength": 4096,
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "instillShortDescription": "The text to generate audio for",
          "instillUIOrder": 1,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Text"
        },
        "voice": {
          "description": "The voice to use when generating the audio. Supported voices are `alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer`.",
          "enum": [
            "alloy",
            "echo",
            "fable",
            "onyx",
            "nova",
            "shimmer"
          ],
          "type": "string",
          "instillAcceptFormats": [
            "string"
          ],
          "default": "alloy",
          "instillShortDescription": "The voice to use when generating the audio",
          "instillUIOrder": 2,
          "instillUpstreamTypes": [
            "value",
            "reference",
            "template"
          ],
          "title": "Voice"
        }
      },
      "required": [
        "text",
        "model",
        "voice"
      ],
      "title": "Input",
      "type": "object"
    },
    "output": {
      "instillUIOrder": 0,
      "properties": {
        "audio": {
          "description": "AI generated audio",
          "instillFormat": "audio/wav",
          "instillUIOrder": 0,
          "title": "Audio",
          "type": "string"
        }
      },
      "required": [],
      "title": "Output",
      "type": "object"
    }
  }
}
