---
title: "Instill Model"
lang: "en-US"
draft: false
description: "Learn about how to set up a VDP Instill Model component https://github.com/instill-ai/instill-core"
---

The Instill Model component is an AI component that allows users to connect the AI models served on the Instill Model Platform.
It can carry out the following tasks:
- [Embedding](#embedding)
- [Chat](#chat)
- [Completion](#completion)
- [Text To Image](#text-to-image)
- [Classification](#classification)
- [Detection](#detection)
- [Keypoint](#keypoint)
- [OCR](#ocr)



## Release Stage

`Alpha`



## Configuration

The component definition and tasks are defined in the [definition.json](https://github.com/instill-ai/pipeline-backend/blob/main/pkg/component/ai/instill/v0/config/definition.json) and [tasks.json](https://github.com/instill-ai/pipeline-backend/blob/main/pkg/component/ai/instill/v0/config/tasks.json) files respectively.






## Supported Tasks

### Embedding

This task refers to the process of generating vector embeddings from input data, which can be text or images. This transformation converts the data into a dense, fixed-length numerical representation that captures the essential features of the original input. These embeddings are typically used in machine learning tasks to represent complex data in a more structured, simplified form.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Input | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| Task ID (required) | `task` | string | `TASK_EMBEDDING` |
| [Data](#embedding-data) (required) | `data` | object | Input data. |
| [Parameter](#embedding-parameter) | `parameter` | object | Input parameter. |
</div>


<details>
<summary> Input Objects in Embedding</summary>

<h4 id="embedding-data">Data</h4>

Input data.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Embeddings](#embedding-embeddings) | `embeddings` | array | List of input data to be embedded.  |
| Model | `model` | string | The model to be used for generating embeddings. It should be `namespace/model-name/version`. i.e. `abrc/yolov7-stomata/v0.1.0`. You can see the version from the Versions tab of Model page.  |
</div>
<h4 id="embedding-parameter">Parameter</h4>

Input parameter.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Dimensions | `dimensions` | integer | Number of dimensions in the output embedding vectors.  |
| Data Format | `format` | string | The data format of the embeddings. Defaults to float.  <br/><details><summary><strong>Enum values</strong></summary><ul><li>`float`</li><li>`base64`</li></ul></details>  |
| Input Type | `input-type` | string | The type of input data to be embedded (e.g., query, document).  |
| Truncate | `truncate` | string | How to handle inputs longer than the max token length. Defaults to 'End'.  <br/><details><summary><strong>Enum values</strong></summary><ul><li>`None`</li><li>`End`</li><li>`Start`</li></ul></details>  |
</div>
</details>

<details>
<summary>The <code>embeddings</code> Object </summary>

<h4 id="embedding-embeddings">Embeddings</h4>

`embeddings` must fulfill one of the following schemas:

<h5 id="embedding-text"><code>Text</code></h5>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Text Content | `text` | string |  When the input is text, the raw text is tokenized and processed into a dense, fixed-length vector that captures semantic information such as word meanings and relationships. These text embeddings enable tasks like sentiment analysis, search, or classification.  |
| Text | `type` | string |  Must be `"text"`   |
</div>

<h5 id="embedding-image-url"><code>Image URL</code></h5>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Image URL | `image-url` | string |  When the input is an image from a URL, the image is first fetched from the URL and then decoded into its original format. It is then processed into a fixed-length vector representing essential visual features like shapes and colors. These image embeddings are useful for tasks like image classification or similarity search, providing structured numerical data for complex visual inputs.  |
| Image URL | `type` | string |  Must be `"image-url"`   |
</div>

<h5 id="embedding-image-base64"><code>Image Base64</code></h5>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Image File | `image-base64` | string |  When the input is an image in base64 format, the base64-encoded data is first decoded into its original image form. The image is then processed and transformed into a dense, fixed-length numerical vector, capturing key visual features like shapes, colors, or textures.  |
| Image File | `type` | string |  Must be `"image-base64"`   |
</div>
</details>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Output | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| [Data](#embedding-data) | `data` | object | Output data. |
</div>

<details>
<summary> Output Objects in Embedding</summary>

<h4 id="embedding-data">Data</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Embeddings](#embedding-embeddings) | `embeddings` | array | List of generated embeddings. |
</div>

<h4 id="embedding-embeddings">Embeddings</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Created | `created` | integer | The Unix timestamp (in seconds) of when the embedding was created. |
| Index | `index` | integer | The index of the embedding vector in the array. |
| Embedding Vector | `vector` | array | The embedding vector. |
</div>
</details>

### Chat

This task involves generating contextually relevant text responses based on input data, such as user queries or prompts. The generated responses include metadata like token usage, which can help track and calculate the cost of the operation. This metadata can also be useful for performance monitoring and analysis.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Input | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| Task ID (required) | `task` | string | `TASK_CHAT` |
| [Chat Data](#chat-chat-data) (required) | `data` | object | The data that will be passed to the chat model. This typically includes the messages exchanged between the user and the assistant, alongside any additional information to guide the conversation. |
| [Input Parameter](#chat-input-parameter) | `parameter` | object | The parameters used to control how the model generates responses. These include settings for token limits, randomness, and the number of response choices. |
</div>


<details>
<summary> Input Objects in Chat</summary>

<h4 id="chat-chat-data">Chat Data</h4>

The data that will be passed to the chat model. This typically includes the messages exchanged between the user and the assistant, alongside any additional information to guide the conversation.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Chat Messages](#chat-chat-messages) | `messages` | array | A collection of individual messages exchanged during the chat session. Each message includes content and other attributes like role and optional participant name.  |
| Model Name | `model` | string | The model to be used for generating chat responses. It should be `namespace/model-name/version`. i.e. `abrc/gpt-3/v0.1.0`. You can see the version from the Versions tab of Model page.  |
</div>
<h4 id="chat-chat-messages">Chat Messages</h4>

A collection of individual messages exchanged during the chat session. Each message includes content and other attributes like role and optional participant name.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Content](#chat-content) | `content` | array | The actual content of the message. It can be a text message, an image URL, or a base64 encoded image.  |
| Name | `name` | string | An optional name for the participant sending the message. This can be used to distinguish between different users or assistants if there are multiple participants with the same role.  |
| Role | `role` | string | Indicates the role of the message sender, such as 'user', 'assistant', or 'system'. This helps the model understand the context in which the message was sent.  <br/><details><summary><strong>Enum values</strong></summary><ul><li>`system`</li><li>`user`</li><li>`assistant`</li></ul></details>  |
</div>
<h4 id="chat-input-parameter">Input Parameter</h4>

The parameters used to control how the model generates responses. These include settings for token limits, randomness, and the number of response choices.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Max New Tokens | `max-tokens` | integer | Defines the maximum number of tokens that the model can generate in response to a given input.  |
| Number of Choices | `n` | integer | Determines how many different response options the model should generate for a single input. This allows the user to choose between multiple potential responses.  |
| Seed | `seed` | integer | A random seed used for controlling the randomness of the model's output. Setting a seed can make the model's responses deterministic and reproducible.  |
| Stream | `stream` | boolean | If set to true, the model will send tokens as they are generated in a streaming manner, rather than waiting for the entire response to be ready.  |
| Temperature | `temperature` | number | A parameter that controls how random or creative the model's responses are. Higher values make the responses more diverse and creative, while lower values make them more focused and deterministic. The default value is 0.7.  |
| Top P | `top-p` | number | An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.  |
</div>
</details>



<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Output | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| [Output Data](#chat-output-data) | `data` | object | This section defines the structure of the output data returned by the chat task. The output includes chat completion choices generated by the model along with metadata such as the reason for finishing, message content, and creation timestamp. Each generated response is structured as a choice that includes details about the model’s reasoning for ending the response generation, the message content, and the role of the message's author. |
| [Output Metadata](#chat-output-metadata) (optional) | `metadata` | object | This section provides metadata about the task completion, including statistics on the usage of tokens. The metadata helps track how many tokens were used in generating the response and how many tokens were included in the input prompt. |
</div>

<details>
<summary> Output Objects in Chat</summary>

<h4 id="chat-output-data">Output Data</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Choices](#chat-choices) | `choices` | array | This is a list of all the possible chat completion choices generated by the model for a given input. Each choice represents a potential response the model can provide based on the input message. |
</div>

<h4 id="chat-choices">Choices</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Created | `created` | integer | The Unix timestamp (in seconds) indicating when the chat completion was generated. This timestamp can be used to track when the response was created. |
| Finish Reason | `finish-reason` | string | The reason why the model stopped generating tokens for this particular response. It could indicate that the model finished the response naturally, or there could be another stopping criterion. |
| Index | `index` | integer | The index or position of this choice in the list of possible responses. This helps to identify which option is being referred to in a set of choices. |
| [Message](#chat-message) | `message` | object | The chat message generated by the model as a response to the input. This includes the text content and the role of the message sender (e.g., user, system, assistant). |
</div>

<h4 id="chat-message">Message</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Content | `content` | string | The actual text content of the generated message. This is what the model suggests as the response based on the given input. |
| Role | `role` | string | The role of the entity that generated the message, such as 'system, 'user', or 'assistant'. This allows differentiation between who is sending the message. |
</div>

<h4 id="chat-output-metadata">Output Metadata</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Usage](#chat-usage) | `usage` | object | Provides information about the number of tokens involved in the request and the completion process. It includes prompt tokens, completion tokens, and the total number of tokens used. |
</div>

<h4 id="chat-usage">Usage</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Completion Tokens | `completion-tokens` | integer | The number of tokens generated in the response. This indicates how many tokens were used in the model's reply. |
| Prompt Tokens | `prompt-tokens` | integer | The number of tokens that were in the input prompt. This counts how many tokens were consumed in processing the initial user query or input. |
| Total Tokens | `total-tokens` | integer | The total number of tokens used in the entire request, which is the sum of prompt tokens and completion tokens. |
</div>
</details>

### Completion

This task generates natural language responses based on a given input prompt. It uses a specified model to predict and create the most appropriate text, such as completing a sentence or generating an entire passage. Users can control the output by setting parameters like maximum tokens (length of the generated text), temperature (creativity level), top-p (nucleus sampling), and number of choices (how many responses to generate). The task supports dynamic input, including system messages that influence the tone or style of the completion, making it versatile for a wide range of applications, from casual text generation to structured outputs like technical documentation or creative writing.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Input | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| Task ID (required) | `task` | string | `TASK_COMPLETION` |
| [Data](#completion-data) (required) | `data` | object | Contains the model name and the prompt for generating a response. This section defines the input prompt and guides the model's behavior using the specified system message. |
| [Parameter](#completion-parameter) | `parameter` | object | Optional settings to control the model’s behavior, including token limit, randomness, and response length. |
</div>


<details>
<summary> Input Objects in Completion</summary>

<h4 id="completion-data">Data</h4>

Contains the model name and the prompt for generating a response. This section defines the input prompt and guides the model's behavior using the specified system message.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Model Name | `model` | string | Name of the model that generates the text response.  |
| Input Prompt | `prompt` | string | The input text or query for which the model will generate a response.  |
| System Message | `system-message` | string | A message sent by the system to guide the model's behavior, affecting tone, style, or other attributes.  |
</div>
<h4 id="completion-parameter">Parameter</h4>

Optional settings to control the model’s behavior, including token limit, randomness, and response length.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Max New okens | `max-tokens` | integer | Specifies the maximum number of tokens the model is allowed to generate for the response.  |
| Number of Choices | `n` | integer | Defines how many response choices the model should generate for each input prompt.  |
| Seed | `seed` | integer | A random seed value used to influence the output. Default is 0, ensuring consistent behavior.  |
| Stream | `stream` | boolean | When true, sends partial token data as it’s generated, instead of waiting for the entire response.  |
| Temperature | `temperature` | number | Controls the randomness of the output. Higher values (closer to 1) make responses more diverse.  |
| Top P | `top-p` | number | Nucleus sampling parameter. It considers only tokens within the cumulative top-p probability mass.  |
</div>
</details>



<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Output | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| [Data](#completion-data) | `data` | object | Contains the generated responses and details like why the model stopped and when the response was generated. |
| [Metadata](#completion-metadata) (optional) | `metadata` | object | Provides information on how many tokens were used in both the input prompt and the generated response. |
</div>

<details>
<summary> Output Objects in Completion</summary>

<h4 id="completion-data">Data</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Choices](#completion-choices) | `choices` | array | List of generated text responses, each with a reason for completion, index, content, and timestamp. |
</div>

<h4 id="completion-choices">Choices</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Content | `content` | string | The text output generated by the model as the response. |
| Created | `created` | integer | Unix timestamp (in seconds) representing when the response was generated. |
| Finish Reason | `finish-reason` | string | Describes why the model stopped generating tokens, such as reaching the limit or completing naturally. |
| Index | `index` | integer | Indicates the position of this choice in the list of generated responses. |
</div>

<h4 id="completion-metadata">Metadata</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Usage](#completion-usage) | `usage` | object | Provides information on how many tokens were used in both the input prompt and the generated response. |
</div>

<h4 id="completion-usage">Usage</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Completion Tokens | `completion-tokens` | integer | The number of tokens used in generating the model's completion. |
| Prompt Tokens | `prompt-tokens` | integer | The number of tokens in the original input prompt provided to the model. |
| Total tokens | `total-tokens` | integer | The total number of tokens used, combining the prompt and the response. |
</div>
</details>

### Text To Image

This task allows users to generate detailed images based on textual descriptions. By providing a well-constructed prompt, users can influence various aspects of the generated image, such as composition, colors, and subjects. The model processes the input text to create an image that matches the description as closely as possible. Additional settings, such as controlling the aspect ratio, limiting unwanted elements via negative prompts, and specifying a seed for repeatability, provide flexibility in generating diverse results. This feature is ideal for visual content creation, creative brainstorming, and conceptual design, offering users the ability to convert ideas into visual representations quickly and efficiently. Whether for art, marketing, or prototyping, this task is a powerful tool for transforming words into visuals.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Input | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| Task ID (required) | `task` | string | `TASK_TEXT_TO_IMAGE` |
| [Input Data](#text-to-image-input-data) (required) | `data` | object | This object contains the necessary data for generating the image. It includes the selected model and the prompt describing the desired elements of the output image. |
| [Parameter](#text-to-image-parameter) | `parameter` | object | This section defines optional parameters that modify the behavior of the image generation, such as aspect ratio, negative prompts, and the number of generated choices. |
</div>


<details>
<summary> Input Objects in Text To Image</summary>

<h4 id="text-to-image-input-data">Input Data</h4>

This object contains the necessary data for generating the image. It includes the selected model and the prompt describing the desired elements of the output image.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Model Name | `model` | string | The model to be used for generating the image. Specify a model that fits your requirements for image generation.  |
| Input Prompt | `prompt` | string | The textual description that will guide the image generation. A strong and detailed prompt leads to better results, clearly specifying subjects, colors, and other visual aspects.  |
</div>
<h4 id="text-to-image-parameter">Parameter</h4>

This section defines optional parameters that modify the behavior of the image generation, such as aspect ratio, negative prompts, and the number of generated choices.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Aspect Ratio | `aspect-ratio` | string | Sets the aspect ratio for the generated image. Defaults to 1:1, but you can choose from other options like 16:9, 4:5, etc.  <br/><details><summary><strong>Enum values</strong></summary><ul><li>`16:9`</li><li>`1:1`</li><li>`21:9`</li><li>`2:3`</li><li>`3:2`</li><li>`4:5`</li><li>`5:4`</li><li>`9:16`</li><li>`9:21`</li></ul></details>  |
| Number of Choices | `n` | integer | The number of image samples to generate for each input prompt. Set the number based on the desired variety of outputs.  |
| Negative Prompt | `negative-prompt` | string | Keywords representing elements you do not want in the generated image, helping to refine the output.  |
| Seed | `seed` | integer | A seed value controls the randomness of the output. Using the same seed will produce consistent results. The default value is 0.  |
</div>
</details>



<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Output | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| [Data](#text-to-image-data) | `data` | object | The data section contains the core output of the image generation task. It includes all relevant information generated by the model, such as the list of created images and their associated metadata, providing a comprehensive result for each input prompt. |
| [Metadata](#text-to-image-metadata) (optional) | `metadata` | object | Provides additional details about the image generation request, such as usage statistics and resource consumption. |
</div>

<details>
<summary> Output Objects in Text To Image</summary>

<h4 id="text-to-image-data">Data</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Choices](#text-to-image-choices) | `choices` | array | A list containing the generated image samples. Each object in the array includes the image (Base64 encoded) and the reason the generation stopped. |
</div>

<h4 id="text-to-image-choices">Choices</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Finish Reason | `finish-reason` | string | Indicates why the model stopped generating tokens, such as reaching success or filtering out inappropriate content. |
| Image | `image` | string | The generated image in Base64 format, ready for use or further processing. |
</div>

<h4 id="text-to-image-metadata">Metadata</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Usage](#text-to-image-usage) | `usage` | object | Tracks how resources were utilized during the request, offering insights into the performance and efficiency of the task. |
</div>
</details>

### Classification

This task focuses on analyzing and categorizing images into predefined groups based on their content. Utilizing advanced machine learning models, this task processes input images and assigns them to specific classes, helping users quickly identify and sort visual data. This capability is particularly valuable in various applications, such as organizing image databases, automating content moderation, or improving search functionalities within image repositories. By leveraging robust algorithms, the Classification task enhances efficiency and accuracy in identifying visual elements, patterns, and anomalies. Users can expect reliable results that streamline workflows and support decision-making processes. With a straightforward input format, this task allows for easy integration into existing systems, empowering users to harness the power of image classification in their projects. Whether for academic research, business intelligence, or creative endeavors, the Classification task serves as a vital tool for transforming images into actionable insights.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Input | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| Task ID (required) | `task` | string | `TASK_CLASSIFICATION` |
| [Input Data](#classification-input-data) (required) | `data` | object | Contains the input image or visual data for classification. It specifies the image format, either via a URL or as a base64 encoded string. This section ensures that the classification model receives the appropriate input, enhancing the accuracy and efficiency of the classification task. |
| [Parameter](#classification-parameter) | `parameter` | object | The parameter property allows for the inclusion of optional settings or configurations that may influence the classification process. While no specific properties are defined, this field enables users to customize the classification task as needed. |
</div>


<details>
<summary> Input Objects in Classification</summary>

</details>

<details>
<summary>The <code>data</code> Object </summary>

<h4 id="classification-data">Data</h4>

`data` must fulfill one of the following schemas:

<h5 id="classification-image-url"><code>Image URL</code></h5>

Specifies the classification model to be utilized for the task. This field must contain a valid model name to ensure accurate classification outcomes.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Input Image URL | `image-url` | string |  Contains the URL of the input image to be classified. This URL must point to a valid image resource for processing.  |
| Model Name | `model` | string |  Specifies the classification model to be utilized for the task. This field must contain a valid model name to ensure accurate classification outcomes.  |
| URL | `type` | string |  Must be `"image-url"`   |
</div>

<h5 id="classification-image-base64"><code>Image Base64</code></h5>

Specifies the classification model to be utilized for the task. This field must contain a valid model name to ensure accurate classification outcomes.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Input Image File | `image-base64` | string |  Contains the base64-encoded string of the input image file to be classified. This format allows for direct submission of image data without requiring an external URL.  |
| Model Name | `model` | string |  Specifies the classification model to be utilized for the task. This field must contain a valid model name to ensure accurate classification outcomes.  |
| File | `type` | string |  Must be `"image-base64"`   |
</div>
</details>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Output | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| [Data](#classification-data) | `data` | object | Contains the results of the classification task, including the predicted category and its associated confidence score. This structured output allows users to understand the model's classification decision and assess its reliability based on the score provided. |
| [Metadata](#classification-metadata) (optional) | `metadata` | object | Contains additional information about the request, including usage statistics. This data provides context and insights regarding the classification process, helping users evaluate performance and resource consumption during the classification task. |
</div>

<details>
<summary> Output Objects in Classification</summary>

<h4 id="classification-data">Data</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Category | `category` | string | Represents the predicted category assigned to the input image. This field provides the model's classification result, helping users identify how the input is categorized. |
| Score | `score` | number | Indicates the confidence score associated with the predicted category, reflecting how certain the model is about its classification. A higher score signifies greater confidence in the assigned category. |
</div>

<h4 id="classification-metadata">Metadata</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Usage](#classification-usage) | `usage` | object | Contains statistics related to the usage of the classification request. This field can provide insights into how resources were utilized during processing, although no specific properties are defined within it. |
</div>
</details>

### Detection

This task focuses on identifying and localizing multiple objects within images, offering essential capabilities for various applications, from autonomous vehicles to surveillance systems. This task employs advanced algorithms to analyze visual data, accurately pinpointing the positions of objects and providing bounding boxes for each detected item. Users can leverage this functionality to enhance object recognition, improve inventory management, and facilitate real-time analysis in diverse environments. By accurately detecting and classifying objects, the system enables more informed decision-making and operational efficiency across industries. Whether it's distinguishing between pedestrians and vehicles or recognizing items on store shelves, the Detection task plays a critical role in computer vision applications, ensuring that machines can interpret and interact with their surroundings effectively. This capability not only optimizes workflows but also contributes to the development of smarter, safer systems in various sectors, including retail, transportation, and robotics.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Input | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| Task ID (required) | `task` | string | `TASK_DETECTION` |
| [Data](#detection-data) (required) | `data` | object | Contains the data required for the detection process. It can include either an image URL or a base64-encoded image representation for detection. |
| [Parameter](#detection-parameter) | `parameter` | object | An object representing any additional parameters for the detection task. This section is currently empty but can be extended for future needs. |
</div>


<details>
<summary> Input Objects in Detection</summary>

</details>

<details>
<summary>The <code>data</code> Object </summary>

<h4 id="detection-data">Data</h4>

`data` must fulfill one of the following schemas:

<h5 id="detection-image-url"><code>Image URL</code></h5>

Specifies the detection model to be utilized for the task. This field must contain a valid model name to ensure accurate detection outcomes.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Input Image URL | `image-url` | string |  Contains the URL of the input image to be classified. This URL must point to a valid image resource for processing.  |
| Model Name | `model` | string |  Specifies the detection model to be utilized for the task. This field must contain a valid model name to ensure accurate detection outcomes.  |
| URL | `type` | string |  Must be `"image-url"`   |
</div>

<h5 id="detection-image-base64"><code>Image Base64</code></h5>

Specifies the detection model to be utilized for the task. This field must contain a valid model name to ensure accurate detection outcomes.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Input Image File | `image-base64` | string |  Contains the base64-encoded string of the input image file to be classified. This format allows for direct submission of image data without requiring an external URL.  |
| Model Name | `model` | string |  Specifies the detection model to be utilized for the task. This field must contain a valid model name to ensure accurate detection outcomes.  |
| File | `type` | string |  Must be `"image-base64"`   |
</div>
</details>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Output | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| [Data](#detection-data) | `data` | object | Contains the results generated by the detection task, including a list of objects detected in the input image along with their details. |
| [Metadata](#detection-metadata) (optional) | `metadata` | object | Contains metadata related to the detection task's output, providing insights into the request's processing and any relevant statistics. |
</div>

<details>
<summary> Output Objects in Detection</summary>

<h4 id="detection-data">Data</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Objects](#detection-objects) | `objects` | array | A list of objects detected in the input image, each with a bounding box, category, and confidence score indicating prediction accuracy. |
</div>

<h4 id="detection-objects">Objects</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Bounding box](#detection-bounding-box) | `bounding-box` | object | Defines the coordinates of the detected object in the format (left, top, width, height). This information specifies the location and size of the detected object. |
| Category | `category` | string | Represents the predicted category assigned to the detected object, providing context about what the object is classified as. |
| Score | `score` | number | The confidence score associated with the predicted category for the detected object, indicating the model's certainty about the classification. |
</div>

<h4 id="detection-bounding-box">Bounding Box</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Height | `height` | number | The height dimension of the detected bounding box, representing its vertical size within the image. |
| Left | `left` | number | The x-axis coordinate representing the left boundary of the bounding box in the image. |
| Top | `top` | number | The y-axis coordinate representing the top boundary of the bounding box in the image. |
| Width | `width` | number | The width dimension of the detected bounding box, representing its horizontal size within the image. |
</div>

<h4 id="detection-metadata">Metadata</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Usage](#detection-usage) | `usage` | object | A section includes statistics on the resource usage for the request. |
</div>
</details>

### Keypoint

Detects and localizes multiple keypoints within objects in images, providing precise coordinates for each keypoint. This task is essential for applications like pose estimation, where key features such as joints or specific object parts need to be identified. By mapping these points, This task enables tracking and analysis of object structure, movement, or posture, making it valuable in fields like human-computer interaction, sports analytics, and robotics. It supports accurate feature extraction, contributing to detailed analysis and improved interaction between systems and visual data.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Input | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| Task ID (required) | `task` | string | `TASK_KEYPOINT` |
| [Data](#keypoint-data) (required) | `data` | object | Contains input data options, allowing for an image URL or a base64-encoded image file for keypoint detection. Each input type requires model selection for processing. |
| [Parameter](#keypoint-parameter) | `parameter` | object | Optional parameters for the keypoint detection task, allowing for adjustments in processing. |
</div>


<details>
<summary> Input Objects in Keypoint</summary>

</details>

<details>
<summary>The <code>data</code> Object </summary>

<h4 id="keypoint-data">Data</h4>

`data` must fulfill one of the following schemas:

<h5 id="keypoint-image-url"><code>Image URL</code></h5>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Input Image URL | `image-url` | string |  URL of the input image for keypoint detection. The image must be accessible online and compatible with the selected model.  |
| Model Name | `model` | string |  Specifies the model for keypoint detection. This model determines the approach and accuracy of the keypoint localization task.  |
| URL | `type` | string |  Must be `"image-url"`   |
</div>

<h5 id="keypoint-image-base64"><code>Image Base64</code></h5>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Input Image File | `image-base64` | string |  Base64-encoded string of the image file for keypoint detection. This format allows direct image data input.  |
| Model Name | `model` | string |  Specifies the model for keypoint detection. This model determines the approach and accuracy of the keypoint localization task.  |
| Image File | `type` | string |  Must be `"image-base64"`   |
</div>
</details>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Output | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| [Data](#keypoint-data) | `data` | object | Contains detected objects and associated keypoints for keypoint detection output. Each object includes its bounding box, keypoints, and confidence score. |
| [Metadata](#keypoint-metadata) (optional) | `metadata` | object | Contains additional metadata for the keypoint detection task output, including request-specific usage information. |
</div>

<details>
<summary> Output Objects in Keypoint</summary>

<h4 id="keypoint-data">Data</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Objects](#keypoint-objects) | `objects` | array | List of detected objects, each containing a bounding box, keypoints list, and a confidence score. |
</div>

<h4 id="keypoint-objects">Objects</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Bounding Box](#keypoint-bounding-box) | `bounding-box` | object | The bounding box defining the outer limits of a detected object, including coordinates for spatial positioning. |
| [Keypoints](#keypoint-keypoints) | `keypoints` | array | A list of predefined keypoints for a detected object, including each point’s x and y coordinates and visibility score. |
| Score | `score` | number | Confidence score indicating the accuracy of the detected object prediction. |
</div>

<h4 id="keypoint-bounding-box">Bounding Box</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Height | `height` | number | Specifies the vertical dimension of the bounding box, representing its height in units of measurement. |
| Left | `left` | number | X-axis coordinate of the left edge of the bounding box, indicating its horizontal starting position. |
| Top | `top` | number | Y-axis coordinate of the top edge of the bounding box, representing its vertical starting position. |
| Width | `width` | number | Horizontal dimension of the bounding box, indicating its width in units of measurement. |
</div>

<h4 id="keypoint-keypoints">Keypoints</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Visibility | `v` | number | Visibility score indicating the likelihood of keypoint presence in the detected object. |
| X Coordinate | `x` | number | X-axis coordinate of the keypoint within the detected object’s bounding box. |
| Y Coordinate | `y` | number | Y-axis coordinate of the keypoint within the detected object’s bounding box. |
</div>

<h4 id="keypoint-metadata">Metadata</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Usage](#keypoint-usage) | `usage` | object | Statistics detailing resource usage for the keypoint detection request, aiding in tracking and monitoring. |
</div>
</details>

### OCR

Optical Character Recognition (OCR) is a process that detects and extracts text from images, transforming it into machine-readable data. This task analyzes visual information to locate and recognize text within various image types, such as scanned documents, photographs, or screenshots. OCR is valuable for digitizing printed or handwritten materials, enabling text searching, editing, and archiving. The task efficiently handles multiple languages and various font styles, ensuring accuracy across different formats. OCR applications include document management, data extraction, and accessibility enhancements, making it a key technology in automating text-based workflows.

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Input | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| Task ID (required) | `task` | string | `TASK_OCR` |
| [Data](#ocr-data) (required) | `data` | object | Contains input data options, allowing for an image URL or a base64-encoded image file for OCR. Each input type requires model selection for processing. |
| [Parameter](#ocr-parameter) | `parameter` | object | The parameter field is an optional object that allows you to provide specific configurations for the OCR task. Although empty by default, it can be populated with parameters that fine-tune processing or control the model’s behavior. These configurations can help adjust the output according to specific OCR requirements or performance optimizations. |
</div>


<details>
<summary> Input Objects in OCR</summary>

</details>

<details>
<summary>The <code>data</code> Object </summary>

<h4 id="ocr-data">Data</h4>

`data` must fulfill one of the following schemas:

<h5 id="ocr-image-url"><code>Image URL</code></h5>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Input Image URL | `image-url` | string |  URL of the input image for OCR. The image must be accessible online and compatible with the selected model.  |
| Model Name | `model` | string |  Specifies the model for OCR. This model determines the approach and accuracy of the OCR task.  |
| URL | `type` | string |  Must be `"image-url"`   |
</div>

<h5 id="ocr-image-base64"><code>Image Base64</code></h5>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Input Image File | `image-base64` | string |  Base64-encoded string of the image file for OCR. This format allows direct image data input.  |
| Model Name | `model` | string |  Specifies the model for OCR. This model determines the approach and accuracy of the OCR task.  |
| Image File | `type` | string |  Must be `"image-base64"`   |
</div>
</details>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Output | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| [Data](#ocr-data) | `data` | object | Contains the structured output data of the OCR task, presenting a list of detected objects, each represented by a bounding box, recognized text, and associated confidence scores. |
| [Metadata](#ocr-metadata) (optional) | `metadata` | object | Contains additional information on the OCR task, including performance metrics and system usage statistics for the request. |
</div>

<details>
<summary> Output Objects in OCR</summary>

<h4 id="ocr-data">Data</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Objects](#ocr-objects) | `objects` | array | A list of detected OCR objects, each specified by a bounding box, recognized text, and a confidence score. |
</div>

<h4 id="ocr-objects">Objects</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Bounding Box](#ocr-bounding-box) | `bounding-box` | object | The bounding box defining the outer limits of a detected object, including coordinates for spatial positioning. |
| Score | `score` | number | Indicates the confidence level of the detected text, scored by the model to reflect recognition accuracy. |
| Text | `text` | string | The recognized text within each detected bounding box. |
</div>

<h4 id="ocr-bounding-box">Bounding Box</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| Height | `height` | number | Specifies the vertical dimension of the bounding box, representing its height in units of measurement. |
| Left | `left` | number | X-axis coordinate of the left edge of the bounding box, indicating its horizontal starting position. |
| Top | `top` | number | Y-axis coordinate of the top edge of the bounding box, representing its vertical starting position. |
| Width | `width` | number | Horizontal dimension of the bounding box, indicating its width in units of measurement. |
</div>

<h4 id="ocr-metadata">Metadata</h4>

<div class="markdown-col-no-wrap" data-col-1 data-col-2>

| Field | Field ID | Type | Note |
| :--- | :--- | :--- | :--- |
| [Usage](#ocr-usage) | `usage` | object | Provides details on resource consumption, such as processing time and model-specific metrics for the OCR request. |
</div>
</details>


