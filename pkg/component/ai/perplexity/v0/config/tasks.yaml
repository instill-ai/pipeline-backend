TASK_CHAT:
  title: Chat
  shortDescription: Generate response base on conversation input.
  input:
    title: Chat Input
    description: Input schema of the chat task.
    shortDescription: Input schema of the chat task.
    properties:
      data:
        title: Chat Data
        description: Input data.
        shortDescription: Input data.
        properties:
          model:
            description: The model to be used for `TASK_CHAT`.
            shortDescription: The model to be used.
            type: string
            enum:
              - sonar
              - sonar-pro

              # Deprecated models, unavailable after 2025-02-22
              - llama-3.1-sonar-small-128k-online
              - llama-3.1-sonar-large-128k-online
              - llama-3.1-sonar-huge-128k-online
            instillCredentialMap:
              values:
                - sonar
                - sonar-pro

                - llama-3.1-sonar-small-128k-online
                - llama-3.1-sonar-large-128k-online
                - llama-3.1-sonar-huge-128k-online
            title: Model Name
            uiOrder: 0
          messages:
            title: Chat Messages
            items:
              properties:
                content:
                  description: The message content.
                  shortDescription: The message content.
                  title: Content
                  items:
                    properties:
                      text:
                        title: Text Message
                        description: Text message.
                        shortDescription: Text message.
                        type: string
                        uiOrder: 1
                      type:
                        title: Text
                        description: Text content type.
                        shortDescription: Text content type.
                        type: string
                        const: text
                        uiOrder: 0
                    required: []
                    title: Text
                    type: object
                  uiOrder: 0
                  type: array
                role:
                  description: The message role, i.e. 'system', 'user' or 'assistant'.
                  shortDescription: The message role, i.e. 'system', 'user' or 'assistant'.
                  type: string
                  title: Role
                  enum:
                    - system
                    - user
                    - assistant
                  uiOrder: 1
                name:
                  description: An optional name for the participant. Provides the model information to differentiate between participants of the same role.
                  shortDescription: An optional name for the participant. Provides the model information to differentiate between participants of the same
                    role.
                  type: string
                  title: Name
                  uiOrder: 2
              required:
                - content
                - role
              type: object
            uiOrder: 1
            description: List of chat messages.
            type: array
        required:
          - messages
        uiOrder: 0
        type: object
      parameter:
        description: Input parameter.
        shortDescription: Input parameter.
        properties:
          max-tokens:
            title: Max New Tokens
            description: The maximum number of completion tokens returned by the API. The total number of tokens requested in max_tokens plus the number
              of prompt tokens sent in messages must not exceed the context window token limit of model requested. If left unspecified, then the model will
              generate tokens until either it reaches its stop token or the end of its context window.
            shortDescription: The maximum number of tokens for model to generate.
            type: integer
            default: 50
            uiOrder: 0
          temperature:
            title: Temperature
            description: The amount of randomness in the response, valued between 0 inclusive and 2 exclusive. Higher values are more random, and lower
              values are more deterministic.
            shortDescription: The temperature for sampling.
            type: number
            default: 0.2
            uiOrder: 1
          top-p:
            title: Top P
            description: The nucleus sampling threshold, valued between 0 and 1 inclusive. For each subsequent token, the model considers the results of
              the tokens with top_p probability mass. We recommend either altering top_k or top_p, but not both.
            shortDescription: Nucleus sampling.
            type: number
            default: 0.9
            uiOrder: 2
          stream:
            title: Stream
            description: If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available.
            shortDescription: If set, partial message deltas will be sent.
            type: boolean
            default: false
            uiOrder: 3
          search-domain-filter:
            title: Search Domain Filter
            description: Given a list of domains, limit the citations used by the online model to URLs from the specified domains. Currently limited to
              only 3 domains for whitelisting and blacklisting. For blacklisting add a `-` to the beginning of the domain string.
            type: string
            uiOrder: 4
          search-recency-filter:
            title: Search Recency Filter
            description: Returns search results within the specified time interval - does not apply to images. Values include `month`, `week`, `day`, `hour`.
            type: string
            uiOrder: 5
          top-k:
            title: Top K
            description: The number of tokens to keep for highest top-k filtering, specified as an integer between 0 and 2048 inclusive. If set to 0, top-k
              filtering is disabled. We recommend either altering top_k or top_p, but not both.
            type: number
            default: 0
            uiOrder: 6
          presence-penalty:
            title: Presence Penalty
            description: A value between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the
              model's likelihood to talk about new topics. Incompatible with `frequency_penalty`.
            type: number
            default: 0
            uiOrder: 7
          frequency-penalty:
            title: Frequency Penalty
            description: A multiplicative penalty greater than 0. Values greater than 1.0 penalize new tokens based on their existing frequency in the text
              so far, decreasing the model's likelihood to repeat the same line verbatim. A value of 1.0 means no penalty. Incompatible with `presence_penalty`.
            type: number
            default: 1
            uiOrder: 8
        required: []
        uiOrder: 1
        title: Input Parameter
        type: object
    required:
      - data
    type: object
  output:
    title: Chat Output
    description: Output schema of the chat task.
    shortDescription: Output schema of the chat task.
    properties:
      data:
        description: Output data.
        shortDescription: Output data.
        properties:
          choices:
            title: Choices
            description: List of chat completion choices.
            shortDescription: List of chat completion choices
            items:
              properties:
                finish-reason:
                  title: Finish Reason
                  description: The reason the model stopped generating tokens.
                  shortDescription: The reason the model stopped generating tokens.
                  uiOrder: 0
                  type: string
                index:
                  title: Index
                  description: The index of the choice in the list of choices.
                  shortDescription: The index of the choice in the list of choices.
                  uiOrder: 1
                  type: integer
                message:
                  title: Message
                  description: A chat message generated by the model.
                  shortDescription: A chat message generated by the model.
                  properties:
                    content:
                      title: Content
                      description: The contents of the message.
                      shortDescription: The contents of the message.
                      uiOrder: 0
                      type: string
                    role:
                      title: Role
                      description: The role of the author of this message.
                      shortDescription: The role of the author of this message.
                      uiOrder: 1
                      type: string
                  required: []
                  uiOrder: 2
                  type: object
                created:
                  title: Created
                  description: 'The timestamp of when the chat completion was created. Format is in ISO 8601. Example: 2024-07-01T11:47:40.388Z.'
                  shortDescription: The Unix timestamp (in seconds) of when the chat completion was created.
                  uiOrder: 3
                  type: integer
              required:
                - finish-reason
                - index
                - message
                - created
              type: object
            uiOrder: 0
            type: array
          citations:
            title: Citations
            description: List of citations.
            shortDescription: List of citations.
            items:
              type: string
            uiOrder: 1
            type: array
        required:
          - choices
        uiOrder: 0
        title: Output Data
        type: object
      metadata:
        description: Output metadata.
        shortDescription: Output metadata.
        properties:
          usage:
            description: Usage statistics for the request.
            shortDescription: Usage statistics for the request.
            properties:
              completion-tokens:
                title: Completion Tokens
                description: Number of tokens in the generated response.
                shortDescription: Number of tokens in the generated response.
                uiOrder: 0
                type: integer
              prompt-tokens:
                title: Prompt Tokens
                description: Number of tokens in the prompt.
                shortDescription: Number of tokens in the prompt.
                uiOrder: 1
                type: integer
              total-tokens:
                title: Total Tokens
                description: Total number of tokens used in the request (prompt + completion).
                shortDescription: Total number of tokens used in the request (prompt + completion).
                uiOrder: 2
                type: integer
            required:
              - completion-tokens
              - prompt-tokens
              - total-tokens
            uiOrder: 0
            title: Usage
            type: object
        required: []
        title: Output Metadata
        uiOrder: 1
        type: object
    required:
      - data
    type: object
